{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dba3bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import boto3\n",
    "import sagemaker\n",
    "import joblib\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import Binarizer, StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sagemaker.amazon.amazon_estimator import RecordSet # could be used if data fits in mem\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cecb61f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/terraform-aws-project-1/notebooks\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60f1e1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "bucket = \"wyatt-datalake\"\n",
    "prefix = \"terraform-aws-project-1\"\n",
    "# BASE_DIR = os.path.dirname(os.path.realpath(__file__))\n",
    "BASE_DIR = \"/home/ec2-user/SageMaker/terraform-aws-project-1/notebooks/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2803cb56",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-63a125389261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     ],\n\u001b[1;32m     25\u001b[0m     \u001b[0mcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBASE_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sklearn_preprocess.py\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mjob_arguments\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"--input-data\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_data\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_data' is not defined"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "\n",
    "processing_instance_type = ParameterString(\n",
    "    name=\"ProcessingInstanceType\", default_value=\"ml.m5.xlarge\"\n",
    ")\n",
    "\n",
    "processing_instance_count = ParameterInteger(name=\"ProcessingInstanceCount\", default_value=1)\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_type=processing_instance_type,\n",
    "    instance_count=processing_instance_count,\n",
    "    base_job_name=f\"{prefix}/sklearn_preprocessor\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    ")\n",
    "step_process = ProcessingStep(\n",
    "    name=\"terraform-aws-project-1\",\n",
    "    processor=sklearn_processor,\n",
    "    outputs=[\n",
    "        ProcessingOutput(output_name=\"train\", source=\"/opt/ml/processing/train\"),\n",
    "        ProcessingOutput(output_name=\"validation\", source=\"/opt/ml/processing/validation\"),\n",
    "        ProcessingOutput(output_name=\"test\", source=\"/opt/ml/processing/test\"),\n",
    "    ],\n",
    "    code=os.path.join(BASE_DIR, \"sklearn_preprocess.py\"),\n",
    "    job_arguments=[\"--input-data\", input_data],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fdb9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop(\"price\", axis=1).values\n",
    "labels = df[\"price\"].values\n",
    "np.random.seed(0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    features, labels, test_size=0.2\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_test, y_test, test_size=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a56cacca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def foo(split, features, labels):\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy='median'),\n",
    "        StandardScaler())\n",
    "\n",
    "    categorical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy='constant', fill_value='missing'),\n",
    "        OneHotEncoder(handle_unknown='ignore'))\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=[\n",
    "            (\"num\", numeric_transformer, make_column_selector(dtype_exclude=\"category\")),\n",
    "            (\"cat\", categorical_transformer, make_column_selector(dtype_include=\"category\"))])\n",
    "    features = preprocessor.fit_transform(df.drop(\"price\", axis=1))\n",
    "    features = features.astype(np.float32)\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, features, target)\n",
    "    buf.seek(0)\n",
    "\n",
    "    #Filename for training data we are uploading to S3 \n",
    "    key = 'linear-data'\n",
    "    #Upload training data to S3\n",
    "    boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, split, key)).upload_fileobj(buf)\n",
    "    s3_split_data = 's3://{}/{}/{}/{}'.format(bucket, prefix, split, key)\n",
    "    print('uploaded {} data location: {}'.format(split, s3_split_data))\n",
    "\n",
    "foo('train', X_train, y_train)\n",
    "foo('test', X_test, y_test)\n",
    "foo('val', X_val, y_val)\n",
    "\n",
    "###Model Artifacts\n",
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('Model Artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3cecf33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uploaded training data location: s3://wyatt-datalake/terraform-aws-project-1/train/linear-train-data\n",
      "uploaded data location: s3://wyatt-datalake/terraform-aws-project-1/test/linear-test-data\n",
      "uploaded data location: s3://wyatt-datalake/terraform-aws-project-1/test/linear-val-data\n",
      "Model Artifacts will be uploaded to: s3://wyatt-datalake/terraform-aws-project-1/output\n"
     ]
    }
   ],
   "source": [
    "###Uploading training data\n",
    "buf = io.BytesIO()\n",
    "smac.write_spmatrix_to_sparse_tensor(buf, X_train, y_train)\n",
    "buf.seek(0)\n",
    "\n",
    "#Filename for training data we are uploading to S3 \n",
    "key = 'linear-train-data'\n",
    "#Upload training data to S3\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', key)).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/train/{}'.format(bucket, prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))\n",
    "\n",
    "###Uploading test data\n",
    "buf = io.BytesIO() # create an in-memory byte array (buf is a buffer I will be writing to)\n",
    "smac.write_spmatrix_to_sparse_tensor(buf, X_test, y_test)\n",
    "buf.seek(0)\n",
    "\n",
    "#Sub-folder for test data\n",
    "key = 'linear-test-data'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'test', key)).upload_fileobj(buf)\n",
    "s3_test_data = 's3://{}/{}/test/{}'.format(bucket, prefix, key)\n",
    "print('uploaded data location: {}'.format(s3_test_data))\n",
    "\n",
    "###Uploading val data\n",
    "buf = io.BytesIO() # create an in-memory byte array (buf is a buffer I will be writing to)\n",
    "smac.write_spmatrix_to_sparse_tensor(buf, X_val, y_val)\n",
    "buf.seek(0)\n",
    "\n",
    "#Sub-folder for val data\n",
    "key = 'linear-val-data'\n",
    "boto3.resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'val', key)).upload_fileobj(buf)\n",
    "s3_val_data = 's3://{}/{}/test/{}'.format(bucket, prefix, key)\n",
    "print('uploaded data location: {}'.format(s3_val_data))\n",
    "\n",
    "###Model Artifacts\n",
    "output_location = 's3://{}/{}/output'.format(bucket, prefix)\n",
    "print('Model Artifacts will be uploaded to: {}'.format(output_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266991d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.image_uris import retrieve\n",
    "\n",
    "ll_image = retrieve(\"linear-learner\", boto3.Session().region_name)\n",
    "\n",
    "ll_estimator = sagemaker.estimator.Estimator(\n",
    "    ll_image,\n",
    "    role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.2xlarge\",\n",
    "    volume_size=20,\n",
    "    max_run=3600,\n",
    "    input_mode=\"Pipe\",\n",
    "    output_path=output_location,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "ll_estimator.set_hyperparameters(predictor_type=\"regressor\", mini_batch_size=32)\n",
    "ll_estimator.fit(inputs={\"train\": s3_train_data}, logs=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b0f038",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelpackage_inference_specification =  {\n",
    "    \"InferenceSpecification\": {\n",
    "      \"Containers\": [\n",
    "         {\n",
    "            \"Image\": '257758044811.dkr.ecr.us-east-2.amazonaws.com/sagemaker-xgboost:1.2-1',\n",
    "         }\n",
    "      ],\n",
    "      \"SupportedContentTypes\": [ \"text/csv\" ],\n",
    "      \"SupportedResponseMIMETypes\": [ \"text/csv\" ],\n",
    "   }\n",
    " }\n",
    "# Specify the model source\n",
    "model_url = \"s3://your-bucket-name/model.tar.gz\"\n",
    "\n",
    "# Specify the model data\n",
    "modelpackage_inference_specification[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"]=model_url\n",
    "\n",
    "create_model_package_input_dict = {\n",
    "    \"ModelPackageGroupName\" : model_package_group_name,\n",
    "    \"ModelPackageDescription\" : \"Model to detect 3 different types of irises (Setosa, Versicolour, and Virginica)\",\n",
    "    \"ModelApprovalStatus\" : \"PendingManualApproval\"\n",
    "}\n",
    "create_model_package_input_dict.update(modelpackage_inference_specification)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
